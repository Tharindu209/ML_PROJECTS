{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout, GlobalAveragePooling2D \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram_path = '/kaggle/input/dl-final-musicgenreclassification-dataset/data/spectrogram'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = os.listdir('/kaggle/input/dl-final-musicgenreclassification-dataset/data/spectrogram/train')\n",
    "\n",
    "# Get the class names\n",
    "classes = os.listdir(data_folder)\n",
    "print(classes)\n",
    "classes.remove('.DS_Store')\n",
    "classes.sort()\n",
    "print(f'Total classes are: {len(classes)}')\n",
    "print(classes)\n",
    "# Show the first image in each folder\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "i = 0\n",
    "\n",
    "for sub_dir in classes:\n",
    "    i+=1\n",
    "    img_file = os.listdir(os.path.join(data_folder,sub_dir))[0]\n",
    "    img_path = os.path.join(data_folder, sub_dir, img_file)\n",
    "    img = mpimg.imread(img_path)\n",
    "    a = fig.add_subplot(1, len(classes), i)\n",
    "    a.axis('off')\n",
    "    imgplot = plt.imshow(img)\n",
    "    a.set_title(img_file)\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Data...\n",
      "Preparing training dataset...\n",
      "Found 661 images belonging to 5 classes.\n",
      "Preparing validation dataset...\n",
      "Found 280 images belonging to 5 classes.\n",
      "Data generators ready\n"
     ]
    }
   ],
   "source": [
    "img_size = (256, 128)\n",
    "batch_size = 10\n",
    "\n",
    "print(\"Getting Data...\")\n",
    "datagen = ImageDataGenerator(rescale=1./255, # normalize pixel values\n",
    "                             validation_split=0.3) # hold back 30% of the images for validation\n",
    "\n",
    "print(\"Preparing training dataset...\")\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    data_folder,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training') # set as training data\n",
    "\n",
    "print(\"Preparing validation dataset...\")\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    data_folder,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation') # set as validation data\n",
    "\n",
    "classnames = list(train_generator.class_indices.keys())\n",
    "print('Data generators ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 255, 127, 256)     3328      \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 85, 42, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 84, 41, 128)       131200    \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 28, 13, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 27, 12, 64)        32832     \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 9, 4, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 9, 4, 64)          0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 11525     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 178,885\n",
      "Trainable params: 178,885\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define a CNN classifier network\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(256, (2, 2), input_shape=train_generator.image_shape, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3,3)))\n",
    "\n",
    "model.add(Conv2D(128, (2, 2), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "\n",
    "model.add(Conv2D(64, (2, 2), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(train_generator.num_classes, activation='softmax'))\n",
    "\n",
    "# With the layers defined, we can now compile the model for categorical (multi-class) classification\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "66/66 [==============================] - 21s 315ms/step - loss: 1.6235 - accuracy: 0.2028 - val_loss: 1.6063 - val_accuracy: 0.2357\n",
      "Epoch 2/15\n",
      "66/66 [==============================] - 21s 320ms/step - loss: 1.6130 - accuracy: 0.2212 - val_loss: 1.6043 - val_accuracy: 0.2107\n",
      "Epoch 3/15\n",
      "66/66 [==============================] - 23s 352ms/step - loss: 1.6061 - accuracy: 0.2089 - val_loss: 1.6011 - val_accuracy: 0.2143\n",
      "Epoch 4/15\n",
      "66/66 [==============================] - 24s 364ms/step - loss: 1.5996 - accuracy: 0.2565 - val_loss: 1.5960 - val_accuracy: 0.2607\n",
      "Epoch 5/15\n",
      "66/66 [==============================] - 23s 342ms/step - loss: 1.5809 - accuracy: 0.2657 - val_loss: 1.5780 - val_accuracy: 0.2714\n",
      "Epoch 6/15\n",
      "66/66 [==============================] - 22s 339ms/step - loss: 1.5692 - accuracy: 0.2611 - val_loss: 1.5769 - val_accuracy: 0.2536\n",
      "Epoch 7/15\n",
      "66/66 [==============================] - 22s 329ms/step - loss: 1.5400 - accuracy: 0.3287 - val_loss: 1.5892 - val_accuracy: 0.2893\n",
      "Epoch 8/15\n",
      "66/66 [==============================] - 22s 330ms/step - loss: 1.5148 - accuracy: 0.3272 - val_loss: 1.5685 - val_accuracy: 0.2857\n",
      "Epoch 9/15\n",
      "66/66 [==============================] - 22s 340ms/step - loss: 1.4992 - accuracy: 0.3241 - val_loss: 1.5709 - val_accuracy: 0.2857\n",
      "Epoch 10/15\n",
      "66/66 [==============================] - 25s 383ms/step - loss: 1.4983 - accuracy: 0.3379 - val_loss: 1.5846 - val_accuracy: 0.2893\n",
      "Epoch 11/15\n",
      "66/66 [==============================] - 24s 356ms/step - loss: 1.4839 - accuracy: 0.3318 - val_loss: 1.5934 - val_accuracy: 0.2929\n",
      "Epoch 12/15\n",
      "66/66 [==============================] - 23s 348ms/step - loss: 1.4563 - accuracy: 0.3641 - val_loss: 1.5736 - val_accuracy: 0.3000\n",
      "Epoch 13/15\n",
      "66/66 [==============================] - 23s 348ms/step - loss: 1.4652 - accuracy: 0.3425 - val_loss: 1.5767 - val_accuracy: 0.2750\n",
      "Epoch 14/15\n",
      "66/66 [==============================] - 23s 346ms/step - loss: 1.4652 - accuracy: 0.3410 - val_loss: 1.5777 - val_accuracy: 0.2857\n",
      "Epoch 15/15\n",
      "66/66 [==============================] - 23s 345ms/step - loss: 1.4674 - accuracy: 0.3794 - val_loss: 1.5755 - val_accuracy: 0.2857\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`y` argument is not supported when using `keras.utils.Sequence` as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m\n\u001b[1;32m      2\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m15\u001b[39m\n\u001b[1;32m      3\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfit(\n\u001b[1;32m      4\u001b[0m     train_generator,\n\u001b[1;32m      5\u001b[0m     steps_per_epoch \u001b[39m=\u001b[39m train_generator\u001b[39m.\u001b[39msamples \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m batch_size,\n\u001b[1;32m      6\u001b[0m     validation_data \u001b[39m=\u001b[39m validation_generator, \n\u001b[1;32m      7\u001b[0m     validation_steps \u001b[39m=\u001b[39m validation_generator\u001b[39m.\u001b[39msamples \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m batch_size,\n\u001b[1;32m      8\u001b[0m     epochs \u001b[39m=\u001b[39m num_epochs)\n\u001b[0;32m----> 9\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(train_generator,validation_generator,batch_size\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m,epochs\u001b[39m=\u001b[39;49m\u001b[39m80\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/keras/engine/data_adapter.py:995\u001b[0m, in \u001b[0;36mKerasSequenceAdapter.__init__\u001b[0;34m(self, x, y, sample_weights, shuffle, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    983\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    984\u001b[0m     x,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    992\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    993\u001b[0m ):\n\u001b[1;32m    994\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_none_or_empty(y):\n\u001b[0;32m--> 995\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    996\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`y` argument is not supported when using \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    997\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`keras.utils.Sequence` as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    998\u001b[0m         )\n\u001b[1;32m    999\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_none_or_empty(sample_weights):\n\u001b[1;32m   1000\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1001\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`sample_weight` argument is not supported when using \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1002\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`keras.utils.Sequence` as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1003\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: `y` argument is not supported when using `keras.utils.Sequence` as input."
     ]
    }
   ],
   "source": [
    "# Train the model over 5 epochs using 30-image batches and using the validation holdout dataset for validation\n",
    "num_epochs = 15\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch = train_generator.samples // batch_size,\n",
    "    validation_data = validation_generator, \n",
    "    validation_steps = validation_generator.samples // batch_size,\n",
    "    epochs = num_epochs)\n",
    "history = model.fit(train_generator,validation_generator,batch_size=16,epochs=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_nums = range(1,num_epochs+1)\n",
    "training_loss = history.history[\"loss\"]\n",
    "validation_loss = history.history[\"val_loss\"]\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(epoch_nums, training_loss)\n",
    "plt.plot(epoch_nums, validation_loss)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['training', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow doesn't have a built-in confusion matrix metric, so we'll use SciKit-Learn\n",
    "\n",
    "print(\"Generating predictions from validation data...\")\n",
    "# Get the image and label arrays for the first batch of validation data\n",
    "x_test = validation_generator[0][0]\n",
    "y_test = validation_generator[0][1]\n",
    "\n",
    "# Use the model to predict the class\n",
    "class_probabilities = model.predict(x_test)\n",
    "\n",
    "# The model returns a probability value for each class\n",
    "# The one with the highest probability is the predicted class\n",
    "predictions = np.argmax(class_probabilities, axis=1)\n",
    "\n",
    "# The actual labels are hot encoded (e.g. [0 1 0], so get the one with the value 1\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classnames))\n",
    "plt.xticks(tick_marks, classnames, rotation=85)\n",
    "plt.yticks(tick_marks, classnames)\n",
    "plt.xlabel(\"Actual Shape\")\n",
    "plt.ylabel(\"Predicted Shape\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "modelFileName = 'song_classifier.pb'\n",
    "model.save(modelFileName)\n",
    "del model  # deletes the existing model variable\n",
    "print('model saved as', modelFileName) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "import numpy as np\n",
    "from random import randint\n",
    "\n",
    "# Function to predict the class of an image\n",
    "def predict_image(classifier, image):\n",
    "    from tensorflow import convert_to_tensor\n",
    "    # The model expects a batch of images as input, so we'll create an array of 1 image\n",
    "    imgfeatures = img.reshape(1, img.shape[0], img.shape[1], img.shape[2])\n",
    "\n",
    "    # We need to format the input to match the training data\n",
    "    # The generator loaded the values as floating point numbers\n",
    "    # and normalized the pixel values, so...\n",
    "    imgfeatures = imgfeatures.astype('float32')\n",
    "    imgfeatures /= 255\n",
    "    \n",
    "    # Use the model to predict the image class\n",
    "    class_probabilities = classifier.predict(imgfeatures)\n",
    "    \n",
    "    # Find the class predictions with the highest predicted probability\n",
    "    index = int(np.argmax(class_probabilities, axis=1)[0])\n",
    "    return index\n",
    "\n",
    "# Create a random test image\n",
    "classnames = os.listdir(os.path.join('/Users/tharindudamruwan/Desktop/ML and AI for DS/Intellihack/Songs', 'Spect'))\n",
    "classnames.sort()\n",
    "img = mpimg.imread(\"/Users/tharindudamruwan/Desktop/ML and AI for DS/Intellihack/Songs/Spect/Calm/3.png\")\n",
    "plt.axis('off')\n",
    "plt.imshow(img)\n",
    "\n",
    "# Use the classifier to predict the class\n",
    "model = models.load_model('song_classifier.h5') # loads the saved model\n",
    "class_idx = predict_image(model, img)\n",
    "print (classnames[class_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
